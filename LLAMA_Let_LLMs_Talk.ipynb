{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate"
      ],
      "metadata": {
        "id": "Br3rwkD92kTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527e3e06-213b-4ebb-fab0-a274fa6308de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting replicate\n",
            "  Downloading replicate-0.25.2-py3-none-any.whl (39 kB)\n",
            "Collecting httpx<1,>=0.21.0 (from replicate)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (24.0)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.11.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.21.0->replicate)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.21.0->replicate)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.18.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.1)\n",
            "Installing collected packages: h11, httpcore, httpx, replicate\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 replicate-0.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_QePkccjnPgnRZZuwpbW4ZUGxyxX1GxA3r2uxw\""
      ],
      "metadata": {
        "id": "raxdQn9dab9v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NtGhYCvv6mJF"
      },
      "outputs": [],
      "source": [
        "import replicate\n",
        "import nltk\n",
        "import re\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topics(path_to_topics):\n",
        "\n",
        "    with open(path_to_topics, \"r\") as r:\n",
        "        data = r.read()\n",
        "\n",
        "    topics = data.strip().split(TOPIC_SPLITTER)\n",
        "    topics = [topic.strip() for topic in topics if len(topic.strip())>0]\n",
        "\n",
        "    return topics"
      ],
      "metadata": {
        "id": "0Vl8Ha_8pKDj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_llama(conversation_Log):\n",
        "    prompt = []\n",
        "    input = {\n",
        "        prompt: conversation_Log\n",
        "    }\n",
        "    output = replicate.run(\n",
        "    \"meta/meta-llama-3-70b-instruct\",\n",
        "    input=input\n",
        ")\n",
        "    response = ''.join(output)\n",
        "    return 'assistant', response.strip()"
      ],
      "metadata": {
        "id": "9sw5HynHpKAb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_res(response_text):\n",
        "    if response_text.startswith(\"Text:\"):\n",
        "        response_text = response_text.lstrip(\"Text:\")\n",
        "\n",
        "    response_text = response_text.strip()\n",
        "    response_text = response_text.rstrip(\",.;:?!\\\"\\'\")\n",
        "    response_text = response_text.lstrip(\",.;:?!\\\"\\'\")\n",
        "\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "OKnjcx31pJ9k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grounded_answers(answer_text, section_text):\n",
        "    sentences = nltk.sent_tokenize(answer_text)\n",
        "    curr_sent = sentences[0]\n",
        "    answers_all = []\n",
        "\n",
        "    for i in range(1, len(sentences)):\n",
        "        x_1 = (curr_sent.strip() + \" \" + sentences[i].strip())\n",
        "        if (x_1 in section_text):\n",
        "            curr_sent = x_1\n",
        "        else:\n",
        "            answers_all.append(curr_sent)\n",
        "            curr_sent = sentences[i]\n",
        "\n",
        "    if not (curr_sent in answers_all):\n",
        "        answers_all.append(curr_sent)\n",
        "\n",
        "    return answers_all"
      ],
      "metadata": {
        "id": "RhpJata9pJ6c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_overlap(answer_text, section_text):\n",
        "    length = len(answer_text.split())\n",
        "    tmp = answer_text\n",
        "    for i in range(0, int(length/2)):\n",
        "        tmp = tmp.split(' ', 1)[1]\n",
        "        if tmp.lower() in section_text.lower():\n",
        "            return True, tmp\n",
        "    return False, None"
      ],
      "metadata": {
        "id": "JJ47IWnopJ3l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_teacher_response(response, topic_background, section):\n",
        "    response = pre_process_res(response)\n",
        "\n",
        "    # condition 0\n",
        "    if (response in cannot_find_text) or (cannot_find_text in response) or (\"the text does not\" in response):\n",
        "        print(\"Condition 0\")\n",
        "        return True, [cannot_find_text]\n",
        "\n",
        "    # condition 1\n",
        "    res, tmp_text = check_overlap(response, section)\n",
        "    if res:\n",
        "        print(\"Condition 1\")\n",
        "        return True, [tmp_text]\n",
        "\n",
        "    # condition 2\n",
        "    clean_section = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", section)\n",
        "    clean_section = ' '.join(clean_section.split())\n",
        "\n",
        "    res, tmp_text = check_overlap(response, clean_section)\n",
        "    if res:\n",
        "        print(\"Condition 2\")\n",
        "        return True, [tmp_text]\n",
        "\n",
        "    # condition 3\n",
        "    sentencess = get_grounded_answers(response, section)\n",
        "\n",
        "    answer_spans = []\n",
        "\n",
        "    for sent in sentencess:\n",
        "        if len(sent) > 0:\n",
        "            sent_clean = pre_process_res(sent)\n",
        "            res, tmp_text = check_overlap(sent_clean, section)\n",
        "            if res:\n",
        "                answer_spans.append(tmp_text)\n",
        "\n",
        "    if len(answer_spans) > 0:\n",
        "        print(\"Condition 3\")\n",
        "        return True, answer_spans\n",
        "\n",
        "    # condition 5\n",
        "    if response in topic_background:\n",
        "        print(\"Condition 4\")\n",
        "        return False, [prompt_answer_from_text]\n",
        "\n",
        "    return False, [prompt_copy_exact_segment]"
      ],
      "metadata": {
        "id": "vrqUNGsGpJ0c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def teacher_process_one_question(index, doc_info, question):\n",
        "\n",
        "    title, background, header, section = doc_info\n",
        "\n",
        "    if index == 0:\n",
        "        prompt_input = first_prompt_teacher.format(title=title, background=background, instruction_teacher= instruction_teacher, header=header, section=section, question=question)\n",
        "\n",
        "    else:\n",
        "        prompt_input = prompt_answer_short.format(question=question)\n",
        "\n",
        "    teacher_conversations.append({'role': 'user', 'content': prompt_input})\n",
        "\n",
        "    for i in range(0, TEACHER_PATIENCE):\n",
        "        role, content = run_llama(teacher_conversations)\n",
        "        teacher_conversations.append({'role': role, 'content': content})\n",
        "        res, res_prompt = check_teacher_response(content, doc_info[1], doc_info[3])\n",
        "\n",
        "        if res == True:\n",
        "            gpt_4_answer = res_prompt\n",
        "            break\n",
        "\n",
        "        elif i < 3:\n",
        "            # ask the given prompt\n",
        "            teacher_conversations.append({'role': 'user', 'content': res_prompt[0]})\n",
        "\n",
        "        elif i == 3:\n",
        "            gpt_4_answer = [\"I cannot find the answer. \" + content]\n",
        "            print(\"final answer didn't match!\")\n",
        "\n",
        "    return gpt_4_answer"
      ],
      "metadata": {
        "id": "23nbM_FGpJx2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_question(question):\n",
        "    if \"\\n\" in question or len(question.split()) > MAX_QUES_LENGTH:\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "tINuf76GpJue"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_student(first_question, title, background, header, prev_answer, prompt_arr_student):\n",
        "\n",
        "    if first_question:\n",
        "        prompt = student_prompt.format(instruction=student_instruction, title=title, background=background, header=header)\n",
        "\n",
        "    elif prev_answer == cannot_find_text:\n",
        "        prompt = random.choice(prompt_arr_student)\n",
        "        print(\"Student selected prompt: \", prompt)\n",
        "        if prompt == interesting_prompt:\n",
        "            prompt_arr_student.remove(interesting_prompt)\n",
        "\n",
        "    else:\n",
        "        prompt = student_prompt_regular\n",
        "\n",
        "    answer_with_prompt = prev_answer + \" \" + prompt\n",
        "    student_conversations.append({'role': 'user', 'content': answer_with_prompt})\n",
        "    role, new_question = run_llama(student_conversations)\n",
        "\n",
        "    while not check_question(new_question):\n",
        "        role, new_question = run_llama(student_conversations)\n",
        "        prompt_2 = answer_with_prompt + \" Please only ask one short question.\"\n",
        "        student_conversations[-1] = {'role': 'user', 'content': prompt_2}\n",
        "\n",
        "    student_conversations.append({'role': role, 'content': new_question})\n",
        "\n",
        "\n",
        "    if prompt == interesting_prompt:\n",
        "        new_question = new_question + \" (tell me maximum number of two segments)\"\n",
        "\n",
        "    return new_question"
      ],
      "metadata": {
        "id": "ODuxOOGMpJox"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def student_teacher_simulation(doc_info):\n",
        "    output_res = {}\n",
        "\n",
        "    prompt_arr_student = [wh_prompt, interesting_prompt, general_prompt, change_aspect]\n",
        "    title, background, header, section = doc_info\n",
        "\n",
        "    output_res['title'] = title\n",
        "    output_res['background'] = background\n",
        "    output_res['header'] = header\n",
        "    output_res['section'] = section\n",
        "    output_res['conversation'] = []\n",
        "\n",
        "\n",
        "    question = simulate_student(True, title, background, header, \"\", prompt_arr_student)\n",
        "    answer_arr = teacher_process_one_question(0, doc_info, question)\n",
        "    tmp= {'question': question,\n",
        "          'answer':answer_arr}\n",
        "    output_res['conversation'].append(tmp)\n",
        "    answer = ' '.join(answer_arr)\n",
        "\n",
        "    for i in range(1, CONV_LENGTH):\n",
        "        question = simulate_student(False, title, background, header, answer, prompt_arr_student)\n",
        "        answer_arr = teacher_process_one_question(i, doc_info, question)\n",
        "        tmp = {'question': question,\n",
        "               'answer': answer_arr}\n",
        "        output_res['conversation'].append(tmp)\n",
        "        answer = ' '.join(answer_arr)\n",
        "\n",
        "    return output_res"
      ],
      "metadata": {
        "id": "GX8YGDg2pJdz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_answer_from_text = \"Please answer from the given section not the given background description.\"\n",
        "prompt_copy_exact_segment = \"Please copy the exact segment from the text.\"\n",
        "cannot_find_text = \"I cannot find the answer.\"\n",
        "prompt_answer_short = \"{question}\\n(Remember that you should select the shortest possible span from the text).\"\n",
        "student_prompt = \"{instruction}\\nBackground:\\n:{title}\\n{background}\\nHeader: {header}\\nPlease start asking questions about: {header}.\"\n",
        "\n",
        "student_instruction = \"\"\"In this task, I am a teacher and have a document, you are a curious student who wants to explore this document by asking questions.\n",
        "The main objective is to learn most of the document that I have. I will give you background knowledge of the document and the title of the document.\n",
        "You should ask questions about this title one by one. When you ask a question, I give you the answer, and then you ask your next question.\n",
        "I’m only allowed to find the answer to your questions from this document, so if I cannot find the answer, I will say “I cannot find the answer, please ask your next question”.\n",
        "You shouldn’t ask questions that can be answered from my previous answers to your previous questions. You should sometimes ask follow-up questions from my previous answers.\"\"\"\n",
        "\n",
        "general_prompt = \"Please ask a general question and don't ask a too specific question.\"\n",
        "wh_prompt = \"Please ask a question starting with where, when, or who.\"\n",
        "interesting_prompt = \"Please ask what is interesting about this document.\"\n",
        "change_aspect = \"Please ask a question about another aspect of the topic.\"\n",
        "student_prompt_regular = \". Please ask your next question.\"\n",
        "prompt_without_ellipsis = \"Please repeat your last question by using Ellipsis and co-references.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "DTO2ZDD2puLz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_topics = \"/content/Cleaned_Topics_Data.txt\"\n",
        "result_path = './simulated_conversations'\n",
        "TOPIC_SPLITTER = \"<Topic: --------------------------->\"\n",
        "\n",
        "TEACHER_PATIENCE = 4\n",
        "MAX_QUES_LENGTH = 25\n",
        "CONV_LENGTH = 12\n",
        "\n",
        "instruction_teacher = \"\"\"In this task, you will be given a text about the topic explained above. You will answer my questions from this text.  Please remember that you cannot generate the answer on your own but should only copy a continuous span from the original text and the copied answer should not exceed 40 tokens.  If you cannot find the answer in the text, please generate ‘I cannot find the answer’.\"\"\"\n",
        "first_prompt_teacher = \"Topic: {title}\\n{background}\\n{instruction_teacher}\\nText:\\n{header}\\n{section}\\nQuestion: {question}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "I8Lyz0mPpxQ8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_data(path_to_data):\n",
        "    with open(path_to_data, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    topics = []\n",
        "    buffer = {}\n",
        "    current_section = None\n",
        "\n",
        "    for line in lines:\n",
        "\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"Topic: \"):\n",
        "            print(\"Reached- Topic\")\n",
        "            print(line)\n",
        "            if buffer:\n",
        "                # Append the collected data before starting a new topic\n",
        "                topics.append([buffer.get(\"Topic\", \"\"), buffer.get(\"Background Information\", \"\"),\n",
        "                               buffer.get(\"Section Header\", \"\"), buffer.get(\"Section Text\", \"\")])\n",
        "                buffer = {}\n",
        "            print(buffer)\n",
        "            current_section = \"Topic\"\n",
        "            buffer[current_section] = line[len(\"Topic:\"):].strip()\n",
        "            print(buffer)\n",
        "            break\n",
        "        elif line.startswith(\"Background Information:\"):\n",
        "\n",
        "            print(\"Reached- BI\")\n",
        "            current_section = \"Background Information\"\n",
        "            buffer[current_section] = line[len(\"Background Information:\"):].strip()\n",
        "        elif line.startswith(\"Section Header:\"):\n",
        "            print(\"Reached- SH\")\n",
        "            # Allow for a new section header irrespective of the current section\n",
        "            current_section = \"Section Header\"\n",
        "            buffer[current_section] = line[len(\"Section Header:\"):].strip()\n",
        "        elif line.startswith(\"Section Text:\"):\n",
        "            print(\"Reached- ST\")\n",
        "            current_section = \"Section Text\"\n",
        "            buffer[current_section] = line[len(\"Section Text:\"):].strip()\n",
        "        elif current_section:  # Append any continuing lines to the current section content\n",
        "            print(\"Reached- next line\")\n",
        "            buffer[current_section] += \" \" + line\n",
        "\n",
        "\n",
        "\n",
        "    # Append the last topic if there is still data in the buffer\n",
        "    if buffer:\n",
        "        topics.append([buffer.get(\"Topic\", \"\"), buffer.get(\"Background Information\", \"\"),\n",
        "                       buffer.get(\"Section Header\", \"\"), buffer.get(\"Section Text\", \"\")])\n",
        "\n",
        "    return topics\n"
      ],
      "metadata": {
        "id": "30IOxbuyzW6W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = preprocess_data(path_to_topics)\n",
        "\n",
        "simulated_convs = []\n",
        "\n",
        "for i in range(0, len(topics)):\n",
        "    teacher_conversations = []\n",
        "    student_conversations = []\n",
        "    doc_info = topics[i]\n",
        "    # doc_info[3] = ' '.join(doc_info[3].split()).rstrip(\"CANNOTANSWER\").strip()\n",
        "    # doc_info[1] = ' '.join(doc_info[1].split())\n",
        "    print(len(doc_info))\n",
        "    print(doc_info)\n",
        "    output_res = student_teacher_simulation(doc_info)\n",
        "    simulated_convs.append(output_res)\n"
      ],
      "metadata": {
        "id": "uASoxcg-p0tg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "4ba666e2-1234-4819-e837-fd1f79503dff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached- Topic\n",
            "Topic: Jennifer Aniston\n",
            "{}\n",
            "{'Topic': 'Jennifer Aniston'}\n",
            "4\n",
            "['Jennifer Aniston', '', '', '']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b16174414aad>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutput_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_teacher_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msimulated_convs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-3b19c09dc451>\u001b[0m in \u001b[0;36mstudent_teacher_simulation\u001b[0;34m(doc_info)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_arr_student\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0manswer_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_process_one_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     tmp= {'question': question,\n",
            "\u001b[0;32m<ipython-input-12-22a1282a0b69>\u001b[0m in \u001b[0;36msimulate_student\u001b[0;34m(first_question, title, background, header, prev_answer, prompt_arr_student)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0manswer_with_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_answer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstudent_conversations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0manswer_with_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_llama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_conversations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2587726b921f>\u001b[0m in \u001b[0;36mrun_llama\u001b[0;34m(conversation_Log)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_llama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation_Log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     input = {\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconversation_Log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     }\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topics))"
      ],
      "metadata": {
        "id": "yvxOaG_h2oEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc_info in topics:\n",
        "  print(\"Reached\")\n",
        "  teacher_conversations = []\n",
        "  student_conversations = []\n",
        "  print(len(doc_info))\n",
        "  output_res = student_teacher_simulation(doc_info)\n",
        "  simulated_convs.append(output_res)\n"
      ],
      "metadata": {
        "id": "amSk7ZSR0-FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(result_path, 'wb') as f:\n",
        "    pickle.dump(simulated_convs)"
      ],
      "metadata": {
        "id": "h9cRdUo_fyHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S6fPKOnP1O_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}